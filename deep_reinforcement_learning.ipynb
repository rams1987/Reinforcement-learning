{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining neural networks and reinforcement learning. \n",
    "1. Use deep neural networks to estimate the q values to each state/Action pair. Input - State and Output - q(s,a1),q(s,a2) etc. \n",
    "2. Stack of consequetive frames are used as input to the network. \n",
    "\n",
    "<img src=\"dqn1.png\"/>\n",
    "\n",
    "\n",
    "Experience replay: \n",
    "we store the agents experience in a dataset called replay memory. We use these replay memory samples to train the network. \n",
    "using the sequential data to train the network will lead to an ineffiecient network.  \n",
    "\n",
    "<img src=\"dqn2.png\"/>\n",
    "\n",
    "\n",
    "1. Initialize replay memory capacity. \n",
    "2. Initialize the network with random weights. \n",
    "3. For each episode: \n",
    "    1. Initialie the starting state:\n",
    "        - via exploration-exploitation strategy. \n",
    "    2. Execute selected action on emulator. \n",
    "    3. Observe reward and next state. \n",
    "    4. Store experience in replay memory. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
